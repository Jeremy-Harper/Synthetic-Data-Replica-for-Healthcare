#  Synthetic-Data-Replica-for-Healthcare

## Description
 
**What is this?**

A tailored hands-on tutorial showing how to use Python to create synthetic data replica from healthcare datasets, in this case we are using a data schema called OMOP, a popular research database for medical outcomes.

**What is a synthetic data replica?**

It's data that is created by an automated process which is intended to have the same statistical patterns as the original dataset but any individual field is generated by an algorithm with no reference to the existing dataset after training. It can be used in healthcare as a way to release data that has no personal information in it. This means you are aggressively protecting patient privacy and exceeding HIPAA compliance by taking it outside a HIPAA environment.

We introduce four types of synthetic build below. 
First creates datasets with the same values as source systems but evenly distributes them (A fast way to share data with trusted partners but eliminating easy reidentification). 
*Comparison of ages in original (left) and random synthetic (right)*
![Random mode age bracket histograms](plots/random_Age_bracket.png)

Second, we create data sets that are generated with independent attribution (Not as useful in healthcare, each column will populate with its appropriate statistics but with no correlation to other columns).
*Comparison of ages in original (left) and correlated synthetic (right)*
![correlated mode age bracket histograms](plots/correlated_Age_bracket.png)

Third correlated attribution between columns. We create a Bayesian Network to keep correlated statistics between all columns. (Often struggles with small count examples, and good with secondary relationships between columns but tertiary relationships between columns may fail)

Fourth Time Series, which need to maintain strict ordering, and relationships between time and measurement values. (Computationally expensive but often how healthcare data needs to be analyzed)

**Who is this tutorial for?**

For any person who wants to be able to generate synthetic replica data from healthcare data.

**Who are you?**

Jeremy Harper, Biomedical Informatician, available to consult

**How can I adopt this technology at my research organization?**

Step 1. Go to your IRB and have them determine that a synthetic data replica is non-human subjects. Have them identify/bless an honest data broker while you are there. As this protects patient privacy and when used properly can expedite useful research and help eliminate paths that might have seen 6 months investment before being able to have data in hand. 

Step 2. Create reports out of a production healthcare instance as the honest broker.

Step 3. Use these modules and make the data available with synthetic data. 

## Overview

In this tutorial you are taking a fake generated version of OMOP and generating synthetic data from it. Its not going to look real, that's unfortunate but its a limitation you're not going to bypass in a github released dataset. 

We are not using actual patient data but are leveraging a synthea OMOP port and generating synthetic data from that.

The practical steps involve:

1. Create a flat file to synthesize: Example Provided at: ./data/COVID19Patients.csv
2. Run anonymization and generate the file: Example Provided at: /data/COVID19Patients_deidentify.csv
3. Generate synthetic dataset 
(Option A) is a dataset with strong correlation between all the data to allow for unpredicted statistical analysis on realistic replica data: ./data/COVID19Patients_data_synthetic_correlated.csv
(Option B) is a dataset with no relation to the real data called random showing examples of all the data that might be in the dataset. Good for software development): ./data/COVID19Patients_data_random.csv 
(Option C) is a dataset with some relation to the real data called Independent each individual column would be representative of the statistics found for that column but will not represent any relationships to other data. My personal opinion is you are unlikely to find this valuable in healthcare targets: ./data/COVID19Patients_data_random.csv 
4. Analyze the synthetic datasets to see how similar they are to the original data found in plots folder.

You may be wondering, why can't we just do the synthetic data step and consider that as anonymization? If it's synthetic surely it won't contain any personal information?

*WARNING* Patterns picked up in the original data can be transferred to the synthetic data. This is especially true for outliers. For instance if there is only one person from an certain area over 85 and this shows up in the synthetic data, we might be able to re-identify them, you need to scrub small cell counts or expand your data appropriately to mask.


## Setup

Commands need to be run from a terminal, Python3 must be installed you can check in terminal with 

```bash
python --version
```

Install required dependent libraries in a virtual environment. You can do that, for example, with venv in ubuntu https://realpython.com/python-virtual-environments-a-primer/

Create the virtual environment. 

```bash
python3 -m venv /path/
```

Activate the new environment you created. 

```bash
source ./venv/bin/activate
```

 (If you are running an M1 Mac you may need to run the following for matlab "brew install pkg-config" and "brew install freetype")

```bash
cd /path/to/repo/synthetic_data_tutorial/
python3 -m venv ./omop_venv
source ./omop_venv/bin/activate
pip3 install -r requirements_freeze.txt
pip3 install -r requirements.txt
sudo apt-get install python3-tk
```
python3-tk: Tk is a Tcl package implemented in C that adds custom commands to create and manipulate GUI widgets.

## Generate mock OMOP dataset

See /Training/OMOP Load.md

## De-identification

If you are going to run this in a production environment please practice de-identification at /Training/deidentification.md

We want to deidentify datasets wherever possible to further lower patient reidentification risks

## Synthetic data replica

In this tutorial we'll create three synthetic datasets, that are synthetic data: *Random*, *Independent*, *Correlated*, *Time Series*

Synthetic data exists on a spectrum from merely the same columns and datatypes as the original data (Random) all the way to carrying approximately all of the statistical patterns of the original dataset (Correlated|Time Series).

**Time Series** uses a different code base from the other three. This is because with time series we need to maintain strict ordering, and relationships between time and measurement values. 

> In **correlated attribute mode**, the software will learn a differentially private Bayesian network capturing the correlation structure between attributes, then draw samples from this model to construct the result dataset.
>
> In cases where the correlated attribute mode is too computationally expensive or when there is insufficient data to derive a reasonable model, one can use an **independent attribute mode**. In this mode, a histogram is derived for each attribute, noise is added to the histogram to achieve differential privacy, and then samples are drawn for each attribute. This is not often helpful in healthcare as we want related variables but its good to know that another technique is available.
>
> For cases where you want no relationship to the patient data, one can use **random mode** that simply generates type-consistent random values for each attribute so if you have 4 zip codes in your database each will be assigned to 25% of the fields.


### Differential privacy and Bayesian networks

We have the option of using differential privacy for anonymization, we are turning it on in a very lightweight manner, if you are using this in production you will want to evaluate your organizations requirements. If you care to learn more about differential privacy this article from accessnow might help (https://www.accessnow.org/understanding-differential-privacy-matters-digital-rights/).

Bayesian networks are graphs with directions which model the statistical relationship between a dataset's variables. It does this by saying certain variables are "parents" of others, that is, their value influences their "children" variables. Parent variables can influence children but children can't influence parents. In our case, if patient age is a parent of waiting time, it means the age of patient influences how long they wait, but how long they doesn't influence their age. So by using Bayesian Networks, we can model these influences and use this model in generating the synthetic data. (https://www.probabilisticworld.com/bayesian-belief-networks-part-1/)

### Notes on Timing

The data generated from synthea didn't have any missingness, it will in a real world dataset. That allows these models to be run VERY VERY quickly. I strongly suggest running the file I've given you FIRST to test whether it is working. To give you an idea of how Degrees of Bayesian network can add time on the rather slow home machine I'm running I've run the following:

With no missingness:
Degree 1 took 30 seconds
Degree 2 degrees its 45
Degree 3 degrees its 120

With Missingness and no deidentification
Degree 1 2000 Seconds (32 Minutes)
Degree 3 2044 Seconds (34 Minutes)



### Random mode

Command to run to generate 10,000 rows of random data from your source dataset

```bash
python3 ./tutorial/synthesize.py     \
--input_file_name='COVID19Patients_deidentify'     \
--mode='random'     \
--row_count=10000
```

Command to generate the same number of rows of data within +-5% of the original, you can crank that up to 50% but be aware that is can half the size of your output dataset as its a random selection. 

```bash
python3 ./tutorial/synthesize.py     \
--input_file_name='COVID19Patients_deidentify'     \
--mode='random'     \
--activate_percentage='Yes'     \
--row_percentage=5
```

Remember Random mode will give you a dataset that has roughly a similar size and that the datatypes and columns align with the source.

#### Attribute Comparison

We'll compare each attribute in the original data to the synthetic data by reviewing the generated plots of histograms. Look in the ./plots folder for the output.

*note, the original dataset is on the left, the synthetic dataset is on the right.

Let's look at the histogram plots now for a few of the attributes. We can see that the generated data is random and doesn't contain any information about averages or distributions.

*Comparison of ages in original data (left) and random synthetic data (right)*
![Random mode age bracket histograms](plots/random_Age_bracket.png)

*Comparison of Ethnicity in original data (left) and random synthetic data (right)*
![Random mode Ethnicity bracket histograms](plots/random_ethnicity_source_value.png)

*Comparison of race in original data (left) and random synthetic data (right)*
![Random mode race bracket histograms](plots/random_race_source_value.png)

*Comparison of all variables via heatmap in original data (left) and random synthetic data (right)*
![Random mode mutual_information bracket histograms](plots/mutual_information_heatmap_random.png)
Learn more about how to read this: http://www.scholarpedia.org/article/Mutual_information)

### Independent attribute mode
*I recommend you skip and go to correlated*
What if we had the use case where we wanted to build models to analyze the medians of ages, or race in the synthetic data but no other variable relationships? In this case we'd use independent attribute mode.

Command to run to generate 10,000 rows of independent data from your source dataset

```bash
python3 ./tutorial/synthesize.py     \
--input_file_name='COVID19Patients_deidentify'     \
--mode='independent'     \
--row_count=10000
```

Command to generate the same number of rows of data within +-5% of the original, you can crank that up to 50% but be aware that is can half the size of your output dataset. 

```bash
python3 ./tutorial/synthesize.py     \
--input_file_name='COVID19Patients_deidentify'     \
--mode='independent'     \
--activate_percentage='Yes'     \
--row_percentage=5
```

Attribute Comparison: Independent

Comparing the attribute histograms we see the independent mode captures the distributions mildly accurately. You can see the synthetic data is somewhat similar but not exactly.


### Correlated attribute mode - include correlations between columns in the data

If we want to capture correlated variables, for instance if patient is related to waiting times, we'll need correlated data. To do this we use *correlated mode*.

#### Data Description: Correlated

There's a couple of parameters that are different here so we'll explain them.

`epsilon_count` is a value for DataSynthesizer's differential privacy which says the amount of noise to add to the data - the higher the value, the more noise and therefore more privacy.

`bayesian_network_degree` is the maximum number of parents in a Bayesian network, i.e., the maximum number of incoming edges. For simplicity's sake, we're going to set this to 1, saying that for a variable only one other variable can influence it. You'll want to crank this higher depending on how many columns your database has to make it more realistic

Command to run to generate 10,000 rows of Correlated data from your source dataset

```bash
python3 ./tutorial/synthesize.py     \
--input_file_name='COVID19Patients_deidentify'     \
--bayesian_network_degree=1     \
--epsilon_count=10     \
--mode='correlated'     \
--row_count=10000
```

Command to generate the same number of rows of data within +-5% of the original, you can crank that up to 50% but be aware that is can half the size of your output dataset. 

```bash
python3 ./tutorial/synthesize.py     \
--input_file_name='COVID19Patients_deidentify'     \
--bayesian_network_degree=1     \
--epsilon_count=10     \
--mode='correlated'     \
--activate_percentage='Yes'     \
--row_percentage=5
```

#### Attribute Comparison: Correlated

We can see correlated mode keeps similar distributions also. It looks the exact same but if you look closely there are small differences in the distributions, crank up epsilon and you'll see bigger disparities.

*Comparison of ages in original data (left) and correlated synthetic data (right)*
![correlated mode age bracket histograms](plots/correlated_Age_bracket.png)

*Comparison of Ethnicity in original data (left) and correlated synthetic data (right)*
![correlated mode Ethnicity bracket histograms](plots/correlated_ethnicity_source_value.png)

*Comparison of race in original data (left) and correlated synthetic data (right)*
![correlated mode race bracket histograms](plots/correlated_race_source_value.png)

*Comparison of all variables via heatmap in original data (left) and correlated synthetic data (right)*
![correlated mode mutual_information bracket histograms](plots/Missingness_Run/mutual_information_heatmap_correlated.png)
Learn more about how to read this: http://www.scholarpedia.org/article/Mutual_information)


#### Custom Run's for other flat files

Data Description Examples that are hardcoded in synthesize.py

The first step is to create a description of the data, defining the datatypes and which are the categorical variables.

Example1:

```python
attribute_to_datatype = {
    'person_id': 'Integer',
    'start_date': 'String', 
    'C19_identification_visit_id': 'String',
    'Gender': 'String', 
    'race_source_value': 'String', 
    'ethnicity_source_value': 'String', 
    'Age bracket': 'String' 
}

attribute_is_categorical = {
    'person_id': False, 
    'start_date': False, 
    'C19_identification_visit_id': False,
    'Gender': True,  
    'race_source_value': True, 
    'ethnicity_source_value': True,     
    'Age bracket': True
}
```

Example2:

```python
attribute_to_datatype = {
    'Time in A&E (mins)': 'Integer',
    'Treatment': 'String',
    'Gender': 'String',
    'Index of Multiple Deprivation Decile': 'Integer',
    'Hospital ID': 'String',
    'Arrival Date': 'String',
    'Arrival hour range': 'String',  
    'Age bracket': 'String'
}

attribute_is_categorical = {
    'Hospital ID': True,
    'Time in A&E (mins)': False,
    'Treatment': True,
    'Gender': True,
    'Index of Multiple Deprivation Decile': False,
    'Arrival Date': True,
    'Arrival hour range': True,  
    'Age bracket': True
}
```

*Note, ideally we'll expand this with a list of expected names/types so that it doesn't need modification to match each flat file.


### Wrap-up

This is the end of the tutorial.


## Credit to others

This tutorial is a tailored, updated version of https://github.com/theodi/synthetic-data-tutorial Which was originally inspired by the [NHS England and ODI Leeds' research](https://odileeds.org/events/synae/) in creating a synthetic dataset from NHS England's accident and emergency admissions. 

The synthetic data generating library they used was [DataSynthetizer]( https://github.com/DataResponsibly/DataSynthesizer) and comes as part of this codebase. 

---
### DataSynthesizer

This is powered through an open source project called, DataSynthesizer. Which is able to generate synthetic datasets of arbitrary size by sampling from the probabilistic model in the dataset description file.

We've created and inspected our synthetic datasets using three modules within it.

> 1. **DataDescriber**: investigates the data types, correlations and distributions of the attributes in the private dataset, and produces a data summary.
> 2. **DataGenerator**: samples from the summary computed by DataDescriber and outputs synthetic data
> 3. **ModelInspector**: creates plots comparing what was computed by DataDescriber, allowing you to evaluate the accuracy of the summarization process

---

Time Series Data Here is a step by step created by Greta AI

https://towardsdatascience.com/creating-synthetic-time-series-data-67223ff08e34


---


### References I thought might be helpful for you

- [Exploring methods for synthetic A&E data](https://odileeds.org/blog/2019-01-24-exploring-methods-for-creating-synthetic-a-e-data) - Jonathan Pearson, NHS England with Open Data Institute Leeds.
- [DataSynthesizer: Privacy-Preserving Synthetic Datasets](https://faculty.washington.edu/billhowe/publications/pdfs/ping17datasynthesizer.pdf) Haoyue Ping, Julia Stoyanovich, and Bill Howe. 2017
